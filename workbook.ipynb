{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Digital Twin (or just a executive assistant) using only local AI on a Macbook Pro\n",
    "\n",
    "Doing this project has been interesting coming from a basis of not having written python for a while and only really having a very high level understanding of how AI works, the paradigms, language, terminology, and so on.\n",
    "\n",
    "I made lots of mistakes, and taken the wrong path on a line of thought, more than I could ever document, and I've learned so much.\n",
    "\n",
    "It's been an interesting journey, of a full week of working on this after work, until 2am, so 40 hours from scratch of \"whats a jupyter nodebook, how do I do that locally?\"\n",
    "\n",
    "I learn through just running at the thing sideways trying to make it happen, I don't pretend to know this all, I'll be doing some crazy things here that are bad, please tell me!\n",
    "\n",
    "## Why local?\n",
    "\n",
    "1. because I can? (or at least that was the assumption)\n",
    "1. I work in uk government, much like any corporate environment, it would not be unreasonable for them to be concerned with me sending heaps of data off to a third party provider whom they have no commercial relationship with, and not also not look to get access to any data that my normal human account couldn't do, or indeed use the in built search tools for example.\n",
    "1. Having played a bit with the cloud vendors offerings, while fabulous and abstract a lot of the heavy lifting I've had to do here, I had no idea how much 40 hours of me playing around and getting things wrong might cost; in fact estimating any AI endevour appears to be a very fluid position in the industry right now, and seems to be focused on cost optimisation (after you've got your first bill presumably).\n",
    "\n",
    "## Ambitions/use cases\n",
    "\n",
    "I want a thing that will let me ask questions like:\n",
    "\n",
    "- Has my access to use gov uk forms been approved?\n",
    "- When was the mailchimp catchup\n",
    "- What does my agenda for today look like?\n",
    "- Am I free at 4pm next thursday?\n",
    "- Who am I seeing today?\n",
    "- Where am I meant to be today?\n",
    "- How long will it take me to get to work\n",
    "- Who was I talking to about snowflake?\n",
    "- Send Jason an email, to tell him that I'm running late 5 minutes late\n",
    "- Search my email and chats to tell me what the 3pm meeting in my calendar is about?\n",
    "- Do I need an umberella today?\n",
    "- What time do I need to leave for work to get to my first meeting?\n",
    "- Will I make it between all my meetings today?\n",
    "- Did I set the bugular alarm at home?\n",
    "- If my wife isn't home, please set the bugular alarm to away, otherwise set it to home\n",
    "\n",
    "Necessary integrations to do that:\n",
    "- ✅ Google Calendar\n",
    "- ✅ Google Mail\n",
    "- ❎ Google Chat\n",
    "- ❎ Google Maps\n",
    "- ❎ Apple Messages\n",
    "- ❎ Apple Notes\n",
    "- ❎ Home Assistant\n",
    "- ❎ WhatsApp\n",
    "- ❎ Date/time converter (e.g. `today` > becomes `2024-04-22`)\n",
    "- ❎ Current/forecast weather based on location\n",
    "- ❎ Math\n",
    "- ❎ Python execution, in a sandbox somewhere, maybe docker?\n",
    "- ❎ A knowledge base of information, e.g. where my office is, my name, my email address, my phone number, etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Go\n",
    "\n",
    "First thing I learned is that you'll need an LLM running\n",
    "\n",
    "I'm using [Ollama](https://www.ollama.com/) to run my llms, think of it a bit like Docker for your llms, they've packaged them up in something that vaguely resembels a Dockerfile OSI model, downloads, pushes them, runs them etc, and presents them as a HTTP interface.\n",
    "\n",
    "When its idle its doing very little, so its safe to leave running all the time, it'll load the model in to memory, run your query and then shuffle it back out of ram when the query is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "%%bash\n",
    "brew install ollama\n",
    "\n",
    "brew services start ollama\n",
    "rm -f token.json chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models.\n",
    "\n",
    "AI has models, this is probably common enough knowledge now given everyone knows about GPT-3, GPT3.5, GPT4, GPT4o and so on, it even gets mainstream press.\n",
    "\n",
    "The thing that wasn't so immediately clear to me was really how they fit, apart from theres ones you can run via a cloud service, like openai/azure/google/bedrock/etc and others that are 'open source' which is a peculiar language in debate, since they're not really open source, not even the rules, weighting, configuration that generated them are open source. They're just freely available to download and use, but as a binary object.\n",
    "\n",
    "The most capable model at time of writing is from Meta `llama3:70b` which is about a 40gb download, and even on a fancy Macbook isn't amazingly responsive, especially if you're used to the speed of chatGPT, it in reviews and stats seems to track at somewhere between GPT3.5 and GPT4 in terms of capability and comprehenson. The stats are nauanced and I'm not really yet sure how they track to the quality of output of what I want.\n",
    "\n",
    "All I'm generally seeing is fast = not so good, slow = better quality response.\n",
    "\n",
    "On the whole I'm going to use `llama3:8b` for querying which is a fair bit smaller, and faster, but still pretty good.\n",
    "\n",
    "I'd like to be able to experiment with smaller/faster llms for some work perhaps, but thats maybe later optimization.I\n",
    "\n",
    "The other thing to consider with models, is some of them only run on nvidia GPUs, some only on CPUs. Given I've got a macbook, I would like to take advantage of the onboard hardware acceleration, which does somewhat limit my choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download models.\n",
    "\n",
    "Now you'll need to pull down some models, the name/tagging is similar to docker of `name:tag`, and the tags are mutable, but provide a hash, theres caching so you can have tweaked/tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 8934d96d3f08... 100% ▕████████████████▏ 3.8 GB                         \n",
      "pulling 8c17c2ebb0ea... 100% ▕████████████████▏ 7.0 KB                         \n",
      "pulling 7c23fb36d801... 100% ▕████████████████▏ 4.8 KB                         \n",
      "pulling 2e0493f67d0c... 100% ▕████████████████▏   59 B                         \n",
      "pulling fa304d675061... 100% ▕████████████████▏   91 B                         \n",
      "pulling 42ba7f8a01dd... 100% ▕████████████████▏  557 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "removing any unused layers \n",
      "success \u001b[?25h\n",
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "removing any unused layers \n",
      "success \u001b[?25h\n",
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 819c2adf5ce6... 100% ▕████████████████▏ 669 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling b837481ff855... 100% ▕████████████████▏   16 B                         \n",
      "pulling 38badd946f91... 100% ▕████████████████▏  408 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "removing any unused layers \n",
      "success \u001b[?25h\n",
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 797b70c4edf8... 100% ▕████████████████▏  45 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling 85011998c600... 100% ▕████████████████▏   16 B                         \n",
      "pulling 548455b72658... 100% ▕████████████████▏  407 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "removing any unused layers \n",
      "success \u001b[?25h\n",
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a... 100% ▕████████████████▏ 4.7 GB                         \n",
      "pulling 4fa551d4f938... 100% ▕████████████████▏  12 KB                         \n",
      "pulling 8ab4849b038c... 100% ▕████████████████▏  254 B                         \n",
      "pulling 577073ffcc6c... 100% ▕████████████████▏  110 B                         \n",
      "pulling 3f8eb4da87fa... 100% ▕████████████████▏  485 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "removing any unused layers \n",
      "success \u001b[?25h\n",
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 0bd51f8f0c97... 100% ▕████████████████▏  39 GB                         \n",
      "pulling 4fa551d4f938... 100% ▕████████████████▏  12 KB                         \n",
      "pulling 8ab4849b038c... 100% ▕████████████████▏  254 B                         \n",
      "pulling 577073ffcc6c... 100% ▕████████████████▏  110 B                         \n",
      "pulling ea8e06d28e47... 100% ▕████████████████▏  486 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "removing any unused layers \n",
      "success \u001b[?25h\n",
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling c55f590f8e55... 100% ▕████████████████▏ 207 MB                         \n",
      "pulling abb2896dc3c2... 100% ▕████████████████▏   11 B                         \n",
      "pulling ff0a35dac3cf... 100% ▕████████████████▏  337 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "removing any unused layers \n",
      "success \u001b[?25h\n",
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 797b70c4edf8... 100% ▕████████████████▏  45 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling 85011998c600... 100% ▕████████████████▏   16 B                         \n",
      "pulling 548455b72658... 100% ▕████████████████▏  407 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "removing any unused layers \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ollama pull llama2:7b\n",
    "ollama pull nomic-embed-text:latest\n",
    "ollama pull mxbai-embed-large:latest\n",
    "ollama pull all-minilm:latest\n",
    "ollama pull llama3:8b\n",
    "ollama pull llama3:70b\n",
    "ollama pull znbang/bge:large-en-v1.5-q4_k_m\n",
    "ollama pull all-minilm:l6-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show installed models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                           \tID          \tSIZE  \tMODIFIED       \n",
      "znbang/bge:large-en-v1.5-q4_k_m\t78c3bcf26504\t207 MB\t8 seconds ago \t\n",
      "all-minilm:l6-v2               \t1b226e2802db\t45 MB \t8 seconds ago \t\n",
      "llama3:70b                     \t786f3184aec0\t39 GB \t9 seconds ago \t\n",
      "llama3:8b                      \t365c0bd3c000\t4.7 GB\t9 seconds ago \t\n",
      "all-minilm:latest              \t1b226e2802db\t45 MB \t10 seconds ago\t\n",
      "mxbai-embed-large:latest       \t468836162de7\t669 MB\t11 seconds ago\t\n",
      "nomic-embed-text:latest        \t0a109f422b47\t274 MB\t11 seconds ago\t\n",
      "llama2:7b                      \t78e26419b446\t3.8 GB\t12 seconds ago\t\n",
      "snowflake-arctic-embed:latest  \t21ab8b9b0545\t669 MB\t3 days ago    \t\n",
      "cns:latest                     \t2572851ad0e7\t39 GB \t7 days ago    \t\n",
      "llama2:latest                  \t78e26419b446\t3.8 GB\t11 days ago   \t\n",
      "m:latest                       \t39704355e5c9\t39 GB \t11 days ago   \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show running models \n",
    "(this should be empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      \tID          \tSIZE \tPROCESSOR\tUNTIL              \n",
      "llama3:70b\t786f3184aec0\t41 GB\t100% GPU \t4 minutes from now\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ollama ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup python env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.embeddings.huggingface_optimum import OptimumEmbedding\n",
    "from llama_index.core.retrievers import VectorIndexAutoRetriever\n",
    "from llama_index.core.vector_stores.types import MetadataInfo, VectorStoreInfo\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display\n",
    "import chromadb\n",
    "from datetime import datetime\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    Settings,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    Document,\n",
    ")\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "import json\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "from llama_index.tools.google.calendar.base import GoogleCalendarToolSpec\n",
    "from llama_index.tools.google.gmail.base import GmailToolSpec\n",
    "from llama_index.core.tools.tool_spec.load_and_search.base import LoadAndSearchToolSpec\n",
    "\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from functools import reduce\n",
    "\n",
    "import helpers as h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll need a database?!\n",
    "\n",
    "AI/LLMs benefit from using a database, apparently, sometimes at least.\n",
    "\n",
    "The preferential database for LLMS appears to be vertex/graph databases rather than a key value, rdms, document store or other.\n",
    "This allows them I think to assimilate/index the data better and then navigate it. (more on indexing in a minute)\n",
    "\n",
    "We're going to use Chroma for this, which is a python thing that persists to sqlite, so its nice and runs in this thread without needing to add more processes and tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = chromadb.PersistentClient(path=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup embedding\n",
    "\n",
    "To create the index, we need to 'embed' the documents, this creates the links between them, which does some sentence splitting and other stuff so that the model can follow the sentiment and create links.\n",
    "\n",
    "All seems very clever, I'll just accept its magic and somehow all works\n",
    "\n",
    "Theres heaps of models, anidotally, some are fast, and optimized to work on my macbook, some only work on nvidia etc just the same as the other models.\n",
    "\n",
    "what it stores is an array with lots of numbers like. The count is set on the `chunk_size` which is `1024` by default\n",
    "```json\n",
    "[0.042785655707120895, -0.02285182662308216, 0.04378447309136391]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = OptimumEmbedding(folder_name=\"./bge_onnx_large\")\n",
    "# Settings.embed_model = OptimumEmbedding(folder_name=\"./bge_onnx_base\")\n",
    "# Settings.embed_model = OllamaEmbedding(model_name=\"znbang/bge:large-en-v1.5-q4_k_m\")\n",
    "# Settings.embed_model = OllamaEmbedding(model_name=\"mxbai-embed-large\")\n",
    "# Settings.embed_model = OllamaEmbedding(model_name=\"llama2\")\n",
    "# Settings.embed_model = OllamaEmbedding(model_name=\"snowflake-arctic-embed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increase that default chunk size to 2048 since some of the docs are too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.chunk_size = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the index\n",
    "I'm going to create an index for my mail, and another for my calendar at this point rather than them being indexed together; I tried this, ultimately the more data you give the poorer accuracy my results were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndex(CollectionName):\n",
    "    chroma_collection = db.get_or_create_collection(CollectionName)\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store, storage_context=storage_context\n",
    "    )\n",
    "    return index\n",
    "\n",
    "cal_index = getIndex(\"calendar\")\n",
    "mail_index = getIndex(\"mail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup default LLM\n",
    "Set the llm that be used if you don't manually specify one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = Ollama(model=\"llama3:8b\", request_timeout=360.0, num_predict=-2, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get authenticated to Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h._get_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the calender events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Number of Calendar Events': 448}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_date = datetime.now() - relativedelta(months=3)\n",
    "end_date = datetime.now() + relativedelta(months=3)\n",
    "\n",
    "calevents = h.load_cal(start_date=start_date, end_date=end_date, number_of_results=2500)\n",
    "\n",
    "display({\"Number of Calendar Events\": len(calevents)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Cal events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e327a522494d9487e76d820e2a64b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cal_index.insert_nodes(calevents, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Number of emails': 1071}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_date = datetime.now() - relativedelta(months=3)\n",
    "\n",
    "emails = h.load_email(start_date=start_date, number_of_results=10000)\n",
    "\n",
    "display({\"Number of emails\": len(emails)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6d161702ae40afaae5e629c9065b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/1071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mail_index.insert_nodes(emails, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain the metadata to the LLM\n",
    "I've not really got this to work reliably if at all, so its marked as skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"calendar events\",\n",
    "    metadata_info=[\n",
    "        MetadataInfo(\n",
    "            name=\"Summary\",\n",
    "            type=\"str\",\n",
    "            description=(\"The title of the event, e.g. 'Catch up with Mr. Smith'\"),\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"Start\",\n",
    "            type=\"str\",\n",
    "            description=(\n",
    "                \"the start time of the event, e.g. '2024-04-02T15:15:00+01:00'\"\n",
    "            ),\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"End\",\n",
    "            type=\"str\",\n",
    "            description=(\"the end time of the event, e.g. '2024-04-02T15:15:00+01:00'\"),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "retriever = VectorIndexAutoRetriever(\n",
    "    cal_index,\n",
    "    vector_store_info=vector_store_info,\n",
    ")\n",
    "display(retriever.retrieve(\"tell me about the Mailchimp catch up\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask Some Questions!\n",
    "I'm probably missing a trick, but i'm not getting great relevancy or accuracy with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = cal_index.as_query_engine(similarity_top_k=20, num_predict=-2)\n",
    "response = query_engine.query(\"mailchimp catchup\")\n",
    "display(response.response)\n",
    "\n",
    "df = pd.DataFrame.from_records(\n",
    "    data=list(map(lambda node: node.metadata, response.source_nodes))\n",
    ")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard query the index without an llm\n",
    "You can however skip using an LLM and just query the database, this tends to be pretty quick, and the results are much more accurate, though will probably be exact\n",
    "\n",
    "You do need to manually embed your query first to give the db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_query = Settings.embed_model.get_query_embedding(\"mailchimp\")\n",
    "result = cal_index.vector_store._collection.query(\n",
    "    query_embeddings=embedded_query, n_results=20\n",
    ")\n",
    "df = pd.DataFrame.from_records(data=result[\"metadatas\"][0])\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search through the mail index too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_query = Settings.embed_model.get_query_embedding(\"forms access\")\n",
    "result = mail_index.vector_store._collection.query(\n",
    "    query_embeddings=embedded_query, n_results=10\n",
    ")\n",
    "df = pd.DataFrame.from_records(data=result[\"metadatas\"][0])\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make some tools for our agent\n",
    "\n",
    "There is a tool specification, and llamaindex have done a great job of making it super easy to do so that the llm is prepped on how to interact with it in a standard format\n",
    "\n",
    "So we can make some tools that will use the LLM to make the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_engine = cal_index.as_query_engine(similarity_top_k=10)\n",
    "mail_engine = mail_index.as_query_engine(similarity_top_k=10)\n",
    "\n",
    "cal_query_tool_cal = QueryEngineTool.from_defaults(\n",
    "    query_engine=cal_engine,\n",
    "    name=\"calendar\",\n",
    "    description=(\n",
    "        f\"Provides information about the user's calendar events for the past three months in the future\"\n",
    "    ),\n",
    ")\n",
    "mail_query_tool_mail = QueryEngineTool.from_defaults(\n",
    "    query_engine=mail_engine,\n",
    "    name=\"email\",\n",
    "    description=(f\"Provides emails to and from the user for the past three months\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector search with a frontend LLM\n",
    "And we can also make some tools that will run a quick search, and then use the LLM to answer the question based on the returned context, this seems to work well, but i don't really understand why it's not good enough to just ask the llm to do the hard work to start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(search_terms, number_of_results_to_return=20, index=None):\n",
    "    embedded_query = Settings.embed_model.get_query_embedding(search_terms)\n",
    "    documents = index.vector_store._collection.query(\n",
    "        query_embeddings=embedded_query, \n",
    "        n_results=cal_index.vector_store._collection.count(), include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "    \n",
    "    return documents[\"documents\"][0][:number_of_results_to_return], documents[\"metadatas\"][0][:number_of_results_to_return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, metadatas = vector_search(\"mailchimp catchup\", index=cal_index)\n",
    "display(pd.DataFrame.from_records(metadatas).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, metadatas = vector_search(\"gov uk forms publisher\", index=mail_index)\n",
    "display(pd.DataFrame.from_records(metadatas).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the provided calendar search results, the most relevant responses to the question \"when is the Mailchimp catchup?\" are:\n",
       "\n",
       "1. **2024-06-11T15:00:00+01:00**: This event has a summary of \"Mailchimp catch up\" and is confirmed.\n",
       "2. **2024-05-14T11:30:00+01:00**: This event has a summary of \"CMP next steps\" which might be related to Mailchimp, but it's not explicitly mentioned as a catchup.\n",
       "\n",
       "Please note that there are no other events with the exact phrase \"Mailchimp catchup\" in their summaries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calsearch(\n",
    "    search_terms, number_of_results_to_consider=5, question=\"summarize the output\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Searches the calendar for relevant documents using the provided search terms and then uses a language model to create a summary based on a provided question.\n",
    "\n",
    "    This method performs a vector-based search to find relevant documents and then constructs a conversation for a large language model (LLM) to generate a response, effectively summarizing the search results based on the given question.\n",
    "\n",
    "    Parameters:\n",
    "    - search_terms (str): The search terms used to find relevant documents in the calendar.\n",
    "    - number_of_results_to_consider (int, optional): The number of top search results to consider for summarization. Defaults to 5.\n",
    "    - question (str, optional): A question or instruction to guide the summarization process. Defaults to \"summarize the output\".\n",
    "\n",
    "    Returns:\n",
    "    - str: The content of the message generated by the LLM that summarizes the top search results.\n",
    "\n",
    "    Raises:\n",
    "    - Exception: If an error occurs during the search or during interaction with the LLM.\n",
    "    \"\"\"\n",
    "    documents, _ = vector_search(\n",
    "        search_terms=search_terms,\n",
    "        number_of_results_to_return=number_of_results_to_consider,\n",
    "        index=cal_index,\n",
    "    )\n",
    "    messages = [\n",
    "        ChatMessage(\n",
    "            role=\"system\",\n",
    "            content=f\"Provide the most relevant responses available to the question:{question}. Given only the calendar search results provided and no prior knowledge\",\n",
    "        ),\n",
    "        ChatMessage(role=\"user\", content=\"\\n --- \\n \".join(documents)),\n",
    "    ]\n",
    "    return Settings.llm.chat(messages).message.content\n",
    "\n",
    "\n",
    "display(Markdown(calsearch(\"mailchimp\", question=\"when is the mailchimp catchup?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mail_vector_search(search_terms, number_of_results_to_return=20):\n",
    "#     embedded_query = Settings.embed_model.get_query_embedding(search_terms)\n",
    "#     documents = mail_index.vector_store._collection.query(\n",
    "#         query_embeddings=embedded_query, \n",
    "#         n_results=cal_index.vector_store._collection.count(), include=[\"documents\", \"metadatas\"]\n",
    "#     )\n",
    "    \n",
    "#     return documents[\"documents\"][0][:number_of_results_to_return], documents[\"metadatas\"][0][:number_of_results_to_return]\n",
    "\n",
    "# _, metadatas = cal_vector_search(\"mailchimp catchup\")\n",
    "# display(pd.DataFrame.from_records(metadatas).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mailsearch(\n",
    "    search_terms,\n",
    "    number_of_results_to_consider=10,\n",
    "    question=\"summarize the each message\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Searches an email index for messages that match the given search terms, then uses a language model to summarize each message.\n",
    "\n",
    "    The function conducts a vector search to retrieve relevant email messages. It then constructs a conversation for the language model to process, asking it to summarize the content of the messages based on the provided question.\n",
    "\n",
    "    Parameters:\n",
    "    - search_terms (str): Keywords or phrases to query the email index.\n",
    "    - number_of_results_to_consider (int, optional): The number of top search results to include for summarization. Defaults to 10.\n",
    "    - question (str, optional): Instruction for the language model to frame the summarization. Defaults to \"summarize each message\".\n",
    "\n",
    "    Returns:\n",
    "    - str: A summary response generated by the language model based on the search results and the question provided.\n",
    "\n",
    "    Raises:\n",
    "    - Exception: If any issue arises during the search or the language model interaction.\n",
    "    \"\"\"\n",
    "    documents, _ = vector_search(\n",
    "        search_terms=search_terms,\n",
    "        number_of_results_to_return=number_of_results_to_consider,\n",
    "        index=mail_index,\n",
    "    )\n",
    "    messages = [\n",
    "        ChatMessage(\n",
    "            role=\"system\",\n",
    "            content=f\"Provide the most relevant response available to the question:{question}. Given only the email search results provided and no prior knowledge\",\n",
    "        ),\n",
    "        ChatMessage(role=\"user\", content=\"\\n --- \\n \".join(documents)),\n",
    "    ]\n",
    "    return Settings.llm.chat(messages).message.content\n",
    "\n",
    "\n",
    "mailsearch(\n",
    "    \"gov uk forms publisher\",\n",
    "    question=\"has Chris's publisher access to use gov uk forms access been approved?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to combine all our robots into some kind of power ranger agent 🤖 >🕵️\n",
    "\n",
    "So now we're in a world, where we can put these tools together, and ask a LLM a question and it'll figure out what tools (might be other llms) to use to help it answer that, keep iterating, until it gets an answer.\n",
    "\n",
    "Neat\n",
    "\n",
    "First lets define some Basic math tools should it need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(*args):\n",
    "    \"\"\"Multiply two or more integers and returns the result integer\"\"\"\n",
    "    return reduce(lambda x, y: x * y, args, 1)\n",
    "\n",
    "\n",
    "def subtract(*args):\n",
    "    \"\"\"subtract one or more integers from the first and returns the result integer\"\"\"\n",
    "    return reduce(lambda x, y: x - y, args, 1)\n",
    "\n",
    "\n",
    "def sum(*args):\n",
    "    \"\"\"add or sum all the integers provided and returns the result integer\"\"\"\n",
    "    return reduce(lambda x, y: x - y, args, 1)\n",
    "\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "sum_tool = FunctionTool.from_defaults(fn=sum)\n",
    "subtract_tool = FunctionTool.from_defaults(fn=subtract)\n",
    "calsearch_tool = FunctionTool.from_defaults(fn=calsearch)\n",
    "mailsearch_tool = FunctionTool.from_defaults(fn=mailsearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove some tools from the provided packages from llamaindex\n",
    "I don't want my agent to actually hit send on emails, i'm also not yet confident I want it to create events, since it can invite people to meetings, and that might be annoying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tool(tool_list, name):\n",
    "    for tool in tool_list:\n",
    "        if tool.metadata.name == name:\n",
    "            tool_list.remove(tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Namespace the name of the query tools\n",
    "Annoyingly theres no namespacing on the tools, so using things from llamaindex can result in an overlap if you want your agent to have both capabilities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def namespace_a_query_tool_list(tool_list, namespace):\n",
    "    for tool in tool_list:\n",
    "        tool.metadata.name = (\n",
    "            f\"{namespace}_{tool.metadata.name}\"\n",
    "            if not tool.metadata.name.startswith(f\"{namespace}_\")\n",
    "            else tool.metadata.name\n",
    "        )\n",
    "    return tool_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load our tool chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>fn_schema</th>\n",
       "      <th>return_direct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sum</th>\n",
       "      <th>sum(*args)\\nadd or sum all the integers provided and returns the result integer</th>\n",
       "      <th>&lt;class 'pydantic.v1.main.sum'&gt;</th>\n",
       "      <th>False</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multiply</th>\n",
       "      <th>multiply(*args)\\nMultiply two or more integers and returns the result integer</th>\n",
       "      <th>&lt;class 'pydantic.v1.main.multiply'&gt;</th>\n",
       "      <th>False</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calsearch</th>\n",
       "      <th>calsearch(search_terms, number_of_results_to_consider=5, question='summarize the output')\\n\\n    Searches the calendar for relevant documents using the provided search terms and then uses a language model to create a summary based on a provided question.\\n\\n    This method performs a vector-based search to find relevant documents and then constructs a conversation for a large language model (LLM) to generate a response, effectively summarizing the search results based on the given question.\\n\\n    Parameters:\\n    - search_terms (str): The search terms used to find relevant documents in the calendar.\\n    - number_of_results_to_consider (int, optional): The number of top search results to consider for summarization. Defaults to 5.\\n    - question (str, optional): A question or instruction to guide the summarization process. Defaults to \"summarize the output\".\\n\\n    Returns:\\n    - str: The content of the message generated by the LLM that summarizes the top search results.\\n\\n    Raises:\\n    - Exception: If an error occurs during the search or during interaction with the LLM.\\n</th>\n",
       "      <th>&lt;class 'pydantic.v1.main.calsearch'&gt;</th>\n",
       "      <th>False</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mailsearch</th>\n",
       "      <th>mailsearch(search_terms, number_of_results_to_consider=10, question='summarize the each message')\\n\\n    Searches an email index for messages that match the given search terms, then uses a language model to summarize each message.\\n\\n    The function conducts a vector search to retrieve relevant email messages. It then constructs a conversation for the language model to process, asking it to summarize the content of the messages based on the provided question.\\n\\n    Parameters:\\n    - search_terms (str): Keywords or phrases to query the email index.\\n    - number_of_results_to_consider (int, optional): The number of top search results to include for summarization. Defaults to 10.\\n    - question (str, optional): Instruction for the language model to frame the summarization. Defaults to \"summarize each message\".\\n\\n    Returns:\\n    - str: A summary response generated by the language model based on the search results and the question provided.\\n\\n    Raises:\\n    - Exception: If any issue arises during the search or the language model interaction.\\n</th>\n",
       "      <th>&lt;class 'pydantic.v1.main.mailsearch'&gt;</th>\n",
       "      <th>False</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gcal_get_date</th>\n",
       "      <th>get_date()\\n\\n        A function to return todays date. Call this before any other functions if you are unaware of the date.\\n</th>\n",
       "      <th>&lt;class 'pydantic.v1.main.get_date'&gt;</th>\n",
       "      <th>False</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gmail_search_messages</th>\n",
       "      <th>search_messages(query: str, max_results: Optional[int] = None)\\n</th>\n",
       "      <th>&lt;class 'pydantic.v1.main.search_messages'&gt;</th>\n",
       "      <th>False</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gmail_create_draft</th>\n",
       "      <th>create_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None) -&gt; str\\nCreate and insert a draft email.\\n           Print the returned draft's message and id.\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            to (Optional[str]): The email addresses to send the message to\\n            subject (Optional[str]): The subject for the event\\n            message (Optional[str]): The message for the event\\n</th>\n",
       "      <th>&lt;class 'pydantic.v1.main.create_draft'&gt;</th>\n",
       "      <th>False</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gmail_update_draft</th>\n",
       "      <th>update_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None, draft_id: str = None) -&gt; str\\nUpdate a draft email.\\n           Print the returned draft's message and id.\\n           This function is required to be passed a draft_id that is obtained when creating messages\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            to (Optional[str]): The email addresses to send the message to\\n            subject (Optional[str]): The subject for the event\\n            message (Optional[str]): The message for the event\\n            draft_id (str): the id of the draft to be updated\\n</th>\n",
       "      <th>&lt;class 'pydantic.v1.main.update_draft'&gt;</th>\n",
       "      <th>False</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gmail_get_draft</th>\n",
       "      <th>get_draft(draft_id: str = None) -&gt; str\\nGet a draft email.\\n           Print the returned draft's message and id.\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            draft_id (str): the id of the draft to be updated\\n</th>\n",
       "      <th>&lt;class 'pydantic.v1.main.get_draft'&gt;</th>\n",
       "      <th>False</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [(sum, sum(*args)\n",
       "add or sum all the integers provided and returns the result integer, <class 'pydantic.v1.main.sum'>, False), (multiply, multiply(*args)\n",
       "Multiply two or more integers and returns the result integer, <class 'pydantic.v1.main.multiply'>, False), (calsearch, calsearch(search_terms, number_of_results_to_consider=5, question='summarize the output')\n",
       "\n",
       "    Searches the calendar for relevant documents using the provided search terms and then uses a language model to create a summary based on a provided question.\n",
       "\n",
       "    This method performs a vector-based search to find relevant documents and then constructs a conversation for a large language model (LLM) to generate a response, effectively summarizing the search results based on the given question.\n",
       "\n",
       "    Parameters:\n",
       "    - search_terms (str): The search terms used to find relevant documents in the calendar.\n",
       "    - number_of_results_to_consider (int, optional): The number of top search results to consider for summarization. Defaults to 5.\n",
       "    - question (str, optional): A question or instruction to guide the summarization process. Defaults to \"summarize the output\".\n",
       "\n",
       "    Returns:\n",
       "    - str: The content of the message generated by the LLM that summarizes the top search results.\n",
       "\n",
       "    Raises:\n",
       "    - Exception: If an error occurs during the search or during interaction with the LLM.\n",
       "    , <class 'pydantic.v1.main.calsearch'>, False), (mailsearch, mailsearch(search_terms, number_of_results_to_consider=10, question='summarize the each message')\n",
       "\n",
       "    Searches an email index for messages that match the given search terms, then uses a language model to summarize each message.\n",
       "\n",
       "    The function conducts a vector search to retrieve relevant email messages. It then constructs a conversation for the language model to process, asking it to summarize the content of the messages based on the provided question.\n",
       "\n",
       "    Parameters:\n",
       "    - search_terms (str): Keywords or phrases to query the email index.\n",
       "    - number_of_results_to_consider (int, optional): The number of top search results to include for summarization. Defaults to 10.\n",
       "    - question (str, optional): Instruction for the language model to frame the summarization. Defaults to \"summarize each message\".\n",
       "\n",
       "    Returns:\n",
       "    - str: A summary response generated by the language model based on the search results and the question provided.\n",
       "\n",
       "    Raises:\n",
       "    - Exception: If any issue arises during the search or the language model interaction.\n",
       "    , <class 'pydantic.v1.main.mailsearch'>, False), (gcal_get_date, get_date()\n",
       "\n",
       "        A function to return todays date. Call this before any other functions if you are unaware of the date.\n",
       "        , <class 'pydantic.v1.main.get_date'>, False), (gmail_search_messages, search_messages(query: str, max_results: Optional[int] = None)\n",
       ", <class 'pydantic.v1.main.search_messages'>, False), (gmail_create_draft, create_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None) -> str\n",
       "Create and insert a draft email.\n",
       "           Print the returned draft's message and id.\n",
       "           Returns: Draft object, including draft id and message meta data.\n",
       "\n",
       "        Args:\n",
       "            to (Optional[str]): The email addresses to send the message to\n",
       "            subject (Optional[str]): The subject for the event\n",
       "            message (Optional[str]): The message for the event\n",
       "        , <class 'pydantic.v1.main.create_draft'>, False), (gmail_update_draft, update_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None, draft_id: str = None) -> str\n",
       "Update a draft email.\n",
       "           Print the returned draft's message and id.\n",
       "           This function is required to be passed a draft_id that is obtained when creating messages\n",
       "           Returns: Draft object, including draft id and message meta data.\n",
       "\n",
       "        Args:\n",
       "            to (Optional[str]): The email addresses to send the message to\n",
       "            subject (Optional[str]): The subject for the event\n",
       "            message (Optional[str]): The message for the event\n",
       "            draft_id (str): the id of the draft to be updated\n",
       "        , <class 'pydantic.v1.main.update_draft'>, False), (gmail_get_draft, get_draft(draft_id: str = None) -> str\n",
       "Get a draft email.\n",
       "           Print the returned draft's message and id.\n",
       "           Returns: Draft object, including draft id and message meta data.\n",
       "\n",
       "        Args:\n",
       "            draft_id (str): the id of the draft to be updated\n",
       "        , <class 'pydantic.v1.main.get_draft'>, False)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine_tools = []\n",
    "\n",
    "query_engine_tools += [sum_tool]\n",
    "query_engine_tools += [multiply_tool]\n",
    "\n",
    "query_engine_tools += [calsearch_tool]\n",
    "query_engine_tools += [mailsearch_tool]\n",
    "GoogleCalendarTool = GoogleCalendarToolSpec()\n",
    "GmailTool = GmailToolSpec()\n",
    "query_engine_tools += namespace_a_query_tool_list(\n",
    "    GoogleCalendarTool.to_tool_list(), \"gcal\"\n",
    ")\n",
    "query_engine_tools += namespace_a_query_tool_list(GmailTool.to_tool_list(), \"gmail\")\n",
    "\n",
    "remove_tool(query_engine_tools, \"gcal_load_data\")\n",
    "remove_tool(query_engine_tools, \"gcal_create_event\")\n",
    "remove_tool(query_engine_tools, \"gmail_send_draft\")\n",
    "remove_tool(query_engine_tools, \"gmail_load_data\")\n",
    "\n",
    "# print(\"Wrapping \" + gsearch_tools[0].metadata.name)\n",
    "# gsearch_load_and_search_tools = LoadAndSearchToolSpec.from_defaults(\n",
    "#     gsearch_tools[0],\n",
    "# ).to_tool_list()\n",
    "\n",
    "# print(\"Wrapping gmail \" + gmail_tools[0].metadata.name)\n",
    "# gmail_load_and_search_tools = LoadAndSearchToolSpec.from_defaults(\n",
    "#     gmail_tools[0],\n",
    "# ).to_tool_list()\n",
    "\n",
    "# print(\"Wrapping google calendar \" + gcal_tools[0].metadata.name)\n",
    "# gcal_load_and_search_tools = LoadAndSearchToolSpec.from_defaults(\n",
    "#     gcal_tools[0],\n",
    "# ).to_tool_list()\n",
    "\n",
    "df = pd.DataFrame.from_records(\n",
    "    data=[tool.metadata.__dict__ for tool in query_engine_tools],\n",
    "    index=[\"name\", \"description\", \"fn_schema\", \"return_direct\"],\n",
    ")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our agent\n",
    "\n",
    "Enabling `verbose` means we see the whole conversation between the agent and its tools, it explains its 'thought' process.\n",
    "\n",
    "Also define a nice helper function to display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "myllm = Ollama(\n",
    "    model=\"llama3:8b\",\n",
    "    # context_window=8096,\n",
    "    num_predict=-2,\n",
    "    # temperature=0.1,\n",
    ")\n",
    "# myllm = Ollama(model=\"llama3:70b\")\n",
    "\n",
    "agent = ReActAgent.from_tools(\n",
    "    query_engine_tools,\n",
    "    llm=myllm,\n",
    "    verbose=True,\n",
    "    request_timeout=3600.0,\n",
    "    max_iterations=100,\n",
    "    context=h.get_system_prompt()\n",
    ")\n",
    "\n",
    "\n",
    "def ask(question, *args, **kwargs):\n",
    "    response = agent.chat(question, *args, **kwargs)\n",
    "    display(Markdown(\"## Response:\"))\n",
    "    display(response.response)\n",
    "    display(Markdown(\"## Tools used:\"))\n",
    "    display(\n",
    "        pd.DataFrame.from_records(\n",
    "            data=[[source.tool_name, source.is_error] for source in response.sources],\n",
    "            columns=[\"Tool\", \"Output\"],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask it some questions!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(\"search my calendar, find out when the mailchimp catch up was?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(\"I would like to know if chris's gov uk forms access been approved?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: I can use a tool to search your calendar.\n",
      "Action: gcal_get_date\n",
      "Action Input: {}\n",
      "\u001b[0m\u001b[1;3;34mObservation: 2024-06-14\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: (Implicit) I can answer without any more tools!\n",
      "Answer: It seems that you have no events scheduled for today, June 14th.\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Response:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'It seems that you have no events scheduled for today, June 14th.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Tools used:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tool</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gcal_get_date</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Tool  Output\n",
       "0  gcal_get_date   False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask(\"search my calendar, to tell me what i've got on today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n",
      "Action: gcal_get_date\n",
      "Action Input: {'date': '2023-06-14'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Error: GoogleCalendarToolSpec.get_date() got an unexpected keyword argument 'date'\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n",
      "Action: gcal_get_date\n",
      "Action Input: {}\n",
      "\u001b[0m\u001b[1;3;34mObservation: 2024-06-14\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: Since the date has been retrieved, I can now search for events on that specific day.\n",
      "Action: calsearch\n",
      "Action Input: {'search_terms': '2024-06-14', 'number_of_results_to_consider': 5, 'question': \"What's scheduled for this day?\"}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Scheduled for this day is:\n",
      "\n",
      "1. Services Week 2024 opening event: This event has already started, and it's scheduled from 11:00 AM to 12:00 PM.\n",
      "2. OCI Fleet Application Management & Launchpad Discussion: This event is confirmed and scheduled for 14:00-15:00 on March 26th.\n",
      "\n",
      "Note that the other events listed are not scheduled for today (March 18).\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: Since I have found the events scheduled for today, I can now report back to Chris with the relevant information.\n",
      "Answer: Hi Chris, based on my calendar search, it appears that you have two events scheduled for today. The first one is the Services Week 2024 opening event, which started at 11:00 AM and is expected to end by 12:00 PM. The second event is the OCI Fleet Application Management & Launchpad Discussion, which is confirmed but not taking place today as it's scheduled for March 26th.\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Response:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"Hi Chris, based on my calendar search, it appears that you have two events scheduled for today. The first one is the Services Week 2024 opening event, which started at 11:00 AM and is expected to end by 12:00 PM. The second event is the OCI Fleet Application Management & Launchpad Discussion, which is confirmed but not taking place today as it's scheduled for March 26th.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Tools used:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tool</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gcal_get_date</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gcal_get_date</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>calsearch</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Tool  Output\n",
       "0  gcal_get_date    True\n",
       "1  gcal_get_date   False\n",
       "2      calsearch   False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask(\"search my calendar to tell me who am I meeting with today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(\"Can you draft a new email to helpdesk and support@example.com about a service outage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools.ondemand_loader_tool import OnDemandLoaderTool\n",
    "from llama_index.readers.wikipedia import WikipediaReader\n",
    "from typing import List\n",
    "\n",
    "from llama_index.readers.google import GmailReader, GoogleCalendarReader\n",
    "\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# reader = WikipediaReader()\n",
    "reader = GmailReader()\n",
    "# tool = OnDemandLoaderTool.from_defaults(\n",
    "#     reader,\n",
    "#     name=\"email\",\n",
    "#     description=\"A tool for searching and retrieving emails\",\n",
    "# )\n",
    "\n",
    "# tool([\"Berlin\"], query_str=\"What's the arts and culture scene in Berlin?\")\n",
    "\n",
    "query_engine_tools = [tool]\n",
    "agent = ReActAgent.from_tools(\n",
    "    query_engine_tools, llm=myllm, verbose=True, max_iterations=30\n",
    ")\n",
    "\n",
    "response = agent.chat(\n",
    "    \"do not try to answer this yourself. use your tools. Has my forms access been approved, explain why you think this\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
